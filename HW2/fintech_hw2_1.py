# -*- coding: utf-8 -*-
"""fintech_hw2_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Um5IqRrIS6BgnoXc_2y6w6aNEqR1M18g

##Construct: DNN
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import keras
from keras.utils import to_categorical
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix
from sklearn import metrics

df = pd.read_csv('Data.csv')

# df.head()

# df.info()

# df.describe()

target_var = 'Class'
features = list(df.columns)
features.remove(target_var)

train_x, test_x, train_y, test_y = train_test_split(df[features], df[target_var], train_size=0.8, test_size=0.2, random_state=7)
# train_y = to_categorical(train_y)
# test_y = to_categorical(test_y)
print(train_x.shape, test_x.shape, train_y.shape, test_y.shape)

"""##1. construct DNN, plot accuracy

Reference

https://www.cnblogs.com/jclian91/p/9777108.html

https://blog.csdn.net/tanlangqie/article/details/81976310

1. cross-entropy error function
2. activation function: sigmoid
3. error backpropagation algorithm using the Adam Optimizer
4. decide the following variables: number of hidden layers, number of hidden units, learning rate, number of iterations and mini-batch size
5. grid search
"""

def create_model(neurons=80):
    model = keras.models.Sequential()
    model.add(keras.layers.Dense(units=neurons, input_dim=30, kernel_initializer='normal', activation='relu')) 
    model.add(keras.layers.Dense(units=1, kernel_initializer='normal', activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) 
    return model

model = KerasClassifier(build_fn=create_model, verbose=0)

# b_size = [1,5,10]
# max_epochs = [20,50,100]
# neurons_select = [60, 70, 80, 90]
# param_grid = dict(batch_size=b_size, nb_epoch=max_epochs, neurons = neurons_select)
# grid = GridSearchCV(estimator=model, param_grid=param_grid) 
# print("Starting training")
# grid_result = grid.fit(train_x, train_y) 
# # h = model.fit(train_x, train_y, batch_size=b_size, epochs=max_epochs, shuffle=True, verbose=1)
# print("Training finished")

# print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_)) 

# display(pd.DataFrame(grid_result.cv_results_).sort_values(by='rank_test_score').head(5))

# print(grid_result.cv_results_)

# for params, mean_test_score in grid_result.cv_results_: 

    # print(mean_test_score, params)

# print(f"最佳準確率: {grid_result.best_score_}，最佳參數組合：{grid_result.best_params_}")
# # 取得 cross validation 的平均準確率及標準差
# means = grid_result.cv_results_['mean_test_score']
# stds = grid_result.cv_results_['std_test_score']
# params = grid_result.cv_results_['params']
# for mean, stdev, param in zip(means, stds, params):
#     print(f"平均準確率: {mean}, 標準差: {stdev}, 參數組合: {param}")

# eval = model.evaluate(test_x, test_y, verbose=0)
# print("Evaluation on test data: loss = %0.6f accuracy = %0.2f%% \n" % (eval[0], eval[1] * 100) )

# predict_y = grid.predict(test_x)
# print(predict_y)

# accuracy = metrics.accuracy_score(test_y, predict_y)
# print(accuracy)

"""Final"""

# model = keras.models.Sequential()
# # Add Input layer, 隱藏層(hidden layer) 有 256個輸出變數
# model.add(keras.layers.Dense(units=80, input_dim=30, kernel_initializer='normal', activation='relu')) 
# # Add output layer
# model.add(keras.layers.Dense(units=1, kernel_initializer='normal', activation='sigmoid'))

# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) 
train_history = model.fit(x=train_x, y=train_y, validation_split=0.2, epochs=50, batch_size=5, verbose=2)

# scores = model.evaluate(test_x, test_y)  
# print(scores)

pred_y = model.predict(test_x)
# y_pred = classifier.predict(X_test)
# print(pred_y)
accuracy = metrics.accuracy_score(test_y, pred_y)
print(accuracy)

# history = model.fit(X, Y, validation_split=0.33, nb_epoch=150, batch_size=10, verbose=0) # list all data in history
# print(history.history.keys())
# summarize history for accuracy
plt.plot(train_history.history['accuracy'])
plt.plot(train_history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left') 
plt.show()

# summarize history for loss 
plt.plot(train_history.history['loss']) 
plt.plot(train_history.history['val_loss']) 
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left') 
plt.show()

"""##3. confusion matrix"""

confmat_test = confusion_matrix(test_y, pred_y)

fig, ax = plt.subplots(figsize=(2.5, 2.5))
ax.matshow(confmat_test, cmap=plt.cm.Blues, alpha=0.3)
for i in range(confmat_test.shape[0]):
    for j in range(confmat_test.shape[1]):
        ax.text(x=j, y=i, s=confmat_test[i,j], va='center', ha='center')
plt.xlabel('predicted label')        
plt.ylabel('true label')
plt.show()

pred_y_train = model.predict(train_x)
confmat_train = confusion_matrix(train_y, pred_y_train)

fig, ax = plt.subplots(figsize=(2.5, 2.5))
ax.matshow(confmat_train, cmap=plt.cm.Blues, alpha=0.3)
for i in range(confmat_train.shape[0]):
    for j in range(confmat_train.shape[1]):
        ax.text(x=j, y=i, s=confmat_train[i,j], va='center', ha='center')
plt.xlabel('predicted label')        
plt.ylabel('true label')
plt.show()

"""##4. For each class, please record precision, recall and F1-score as well as the averages of those criteria over all classes in your report."""

#class_0
TP = confmat_test[0][0]
FP = confmat_test[1][0]
FN = confmat_test[0][1]
TN = confmat_test[1][1]

precision = TP/(TP+FP)
recall = TP/(TP+FN)
F1_score = 2 * precision * recall / (precision + recall)
print(precision, recall, F1_score)

#class_1
TN = confmat_test[0][0]
FN = confmat_test[1][0]
FP = confmat_test[0][1]
TP = confmat_test[1][1]

precision = TP/(TP+FP)
recall = TP/(TP+FN)
F1_score = 2 * precision * recall / (precision + recall)
print(precision, recall, F1_score)

"""##5.  plot learning curve, receiver operating characteristic curve (ROC, as shown in Figure 3) and precision-recall curve (PRC, as shown in Figure 3) with their area-under-curve (AUROC and AUPRC) for DNN, decision tree and random forest."""

ns_probs = [0 for _ in range(len(test_y))]
# # fit a model
# model = LogisticRegression(solver='lbfgs')
# model.fit(trainX, trainy)
# predict probabilities
lr_probs = model.predict_proba(test_x)
# keep probabilities for the positive outcome only
lr_probs = lr_probs[:, 1]
# calculate scores
ns_auc = metrics.roc_auc_score(test_y, ns_probs)
lr_auc = metrics.roc_auc_score(test_y, lr_probs)
# summarize scores
print('No Skill: ROC AUC=%.3f' % (ns_auc))
print('Logistic: ROC AUC=%.3f' % (lr_auc))
# calculate roc curves
ns_fpr, ns_tpr, _ = metrics.roc_curve(test_y, ns_probs)
lr_fpr, lr_tpr, _ = metrics.roc_curve(test_y, lr_probs)
# plot the roc curve for the model
plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')
plt.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')
# axis labels
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
# show the legend
plt.legend()
# show the plot
plt.show()

# test_yy = pd.get_dummies(test_y)
# test_yy[0]

# ns_probs = [0 for _ in range(len(test_yy[0]))]
# # # fit a model
# # model = LogisticRegression(solver='lbfgs')
# # model.fit(trainX, trainy)
# # predict probabilities
# lr_probs = model.predict_proba(test_x)
# # keep probabilities for the positive outcome only
# lr_probs = lr_probs[:, 0]
# # calculate scores
# ns_auc = metrics.roc_auc_score(test_yy[0], ns_probs)
# lr_auc = metrics.roc_auc_score(test_yy[0], lr_probs)
# # summarize scores
# print('No Skill: ROC AUC=%.3f' % (ns_auc))
# print('Logistic: ROC AUC=%.3f' % (lr_auc))
# # calculate roc curves
# ns_fpr, ns_tpr, _ = metrics.roc_curve(test_yy[0], ns_probs)
# lr_fpr, lr_tpr, _ = metrics.roc_curve(test_yy[0], lr_probs)
# # plot the roc curve for the model
# plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')
# plt.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')
# # axis labels
# plt.xlabel('False Positive Rate')
# plt.ylabel('True Positive Rate')
# # show the legend
# plt.legend()
# # show the plot
# plt.show()

lr_probs = model.predict_proba(test_x)
# keep probabilities for the positive outcome only
lr_probs = lr_probs[:, 1]
# predict class values
# yhat = model.predict(testX)
lr_precision, lr_recall, _ = metrics.precision_recall_curve(test_y, lr_probs)
lr_auc = metrics.auc(lr_recall, lr_precision)
# summarize scores
print('Logistic: auc=%.3f' % lr_auc)
# plot the precision-recall curves
no_skill = len(test_y[test_y==1]) / len(test_y)
plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')
plt.plot(lr_recall, lr_precision, marker='.', label='Logistic')
# axis labels
plt.xlabel('Recall')
plt.ylabel('Precision')
# show the legend
plt.legend()
# show the plot
plt.show()

# lr_probs = model.predict_proba(test_x)
# # keep probabilities for the positive outcome only
# lr_probs = lr_probs[:, 0]
# # predict class values
# # yhat = model.predict(testX)
# lr_precision, lr_recall, _ = metrics.precision_recall_curve(test_yy[0], lr_probs)
# lr_auc = metrics.auc(lr_recall, lr_precision)
# # summarize scores
# print('Logistic: auc=%.3f' % lr_auc)
# # plot the precision-recall curves
# no_skill = len(test_yy[0][test_yy[0]==1]) / len(test_yy[0])
# plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')
# plt.plot(lr_recall, lr_precision, marker='.', label='Logistic')
# # axis labels
# plt.xlabel('Recall')
# plt.ylabel('Precision')
# # show the legend
# plt.legend()
# # show the plot
# plt.show()

"""##6.Please draw the lift curve of models in problem 1: DNN, decision tree and random forest. Please also draw the ideal lift curve and random guess like the above figure."""

def get_cum_gains(df, score, target, title, cm):
    df1 = df[[score,target]].dropna()

    TP = cm[0][0]
    FN = cm[0][1]
    FP = cm[1][0]
    TN = cm[1][1]
    fpr, tpr, thresholds = metrics.roc_curve(df1[target], df1[score])
    # ppr=(tpr*df[target].sum()+fpr*(df[target].count()-df[target].sum()))/df[target].count()
    ppr=(tpr*df[target].sum()+fpr*(df[target].count()-df[target].sum()))
    print(ppr)
    print(tpr)

    ttt = [i*(TP+FN) for i in tpr]
    print(ttt)
    # plt.figure(figsize=(12,4))
    # plt.subplot(1,2,1)
    # metrics.recall_score

    plt.plot(ppr, ttt, label='')
    plt.plot([0,df[target].count()], [0,ttt[-1]])
    plt.plot([0, FN+TN,df[target].count()],[0, ttt[-1],ttt[-1]])
    # plt.plot([0, 1], [0, 1], 'k--')
    # plt.xlim([0.0, 1.0])
    # plt.ylim([0.0, 1.05])
    plt.grid(b=True, which='both', color='0.65',linestyle='-')
    plt.xlabel('%Population')
    plt.ylabel('%Target')
    plt.title(title+'Cumulative Gains Chart')
    plt.legend(loc="lower right")
    # plt.subplot(1,2,2)
    # plt.plot(ppr, tpr/ppr, label='')
    # plt.plot([0, 1], [1, 1], 'k--')
    # plt.grid(b=True, which='both', color='0.65',linestyle='-')
    # plt.xlabel('%Population')
    # plt.ylabel('Lift')
    # plt.title(title+'Lift Curve')

lr_probs = model.predict_proba(test_x)
lr_probs = lr_probs[:, 1]
dict_s = {"score": lr_probs}
cg = pd.DataFrame(dict_s)
test_y_ri = test_y.reset_index()
test_y_ri = test_y_ri['Class']
cg['target'] = test_y_ri
pred_y_test = model.predict(test_x)
confmat = confusion_matrix(test_y, pred_y_test)
get_cum_gains(cg, 'score', 'target', 'title', confmat)

# !pip install scikit-plot

# import scikitplot as skplt
# y_probas = model.predict_proba(test_x)
# skplt.metrics.plot_cumulative_gain(test_y, y_probas)
# plt.show()