# -*- coding: utf-8 -*-
"""fintech_hw2_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/146VehKcTw0ZvC2xyk2V_FtjCRmMINe-Y

##Construct: Random Forest
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
# from sklearn.tree import DecisionTreeClassifier
from sklearn import ensemble, metrics
from sklearn.metrics import confusion_matrix
import keras

df = pd.read_csv('Data.csv')

target_var = 'Class'
features = list(df.columns)
features.remove(target_var)

train_x, test_x, train_y, test_y = train_test_split(df[features], df[target_var], train_size=0.8, test_size=0.2, random_state=7)
# train_y = to_categorical(train_y)
# test_y = to_categorical(test_y)
print(train_x.shape, test_x.shape, train_y.shape, test_y.shape)

"""##1. Construct Random Forest, Plot Accuracy"""

model = ensemble.RandomForestClassifier(n_estimators = 100)
forest_fit = model.fit(train_x, train_y)

pred_y = model.predict(test_x)

accuracy = metrics.accuracy_score(test_y, pred_y)
print(accuracy)

"""##3. confusion matrix"""

confmat_test = confusion_matrix(test_y, pred_y)

fig, ax = plt.subplots(figsize=(2.5, 2.5))
ax.matshow(confmat_test, cmap=plt.cm.Blues, alpha=0.3)
for i in range(confmat_test.shape[0]):
    for j in range(confmat_test.shape[1]):
        ax.text(x=j, y=i, s=confmat_test[i,j], va='center', ha='center')
plt.xlabel('predicted label')        
plt.ylabel('true label')
plt.show()

pred_y_train = model.predict(train_x)
confmat_train = confusion_matrix(train_y, pred_y_train)

fig, ax = plt.subplots(figsize=(2.5, 2.5))
ax.matshow(confmat_train, cmap=plt.cm.Blues, alpha=0.3)
for i in range(confmat_train.shape[0]):
    for j in range(confmat_train.shape[1]):
        ax.text(x=j, y=i, s=confmat_train[i,j], va='center', ha='center')
plt.xlabel('predicted label')        
plt.ylabel('true label')
plt.show()

"""##4. For each class, please record precision, recall and F1-score as well as the averages of those criteria over all classes in your report."""

#class_0
TP = confmat_test[0][0]
FP = confmat_test[1][0]
FN = confmat_test[0][1]
TN = confmat_test[1][1]

precision = TP/(TP+FP)
recall = TP/(TP+FN)
F1_score = 2 * precision * recall / (precision + recall)
print(precision, recall, F1_score)

#class_1
TN = confmat_test[0][0]
FN = confmat_test[1][0]
FP = confmat_test[0][1]
TP = confmat_test[1][1]

precision = TP/(TP+FP)
recall = TP/(TP+FN)
F1_score = 2 * precision * recall / (precision + recall)
print(precision, recall, F1_score)

"""##5.  plot learning curve, receiver operating characteristic curve (ROC, as shown in Figure 3) and precision-recall curve (PRC, as shown in Figure 3) with their area-under-curve (AUROC and AUPRC) for DNN, decision tree and random forest."""

ns_probs = [0 for _ in range(len(test_y))]
# # fit a model
# model = LogisticRegression(solver='lbfgs')
# model.fit(trainX, trainy)
# predict probabilities
lr_probs = model.predict_proba(test_x)
# keep probabilities for the positive outcome only
lr_probs = lr_probs[:, 1]
# calculate scores
ns_auc = metrics.roc_auc_score(test_y, ns_probs)
lr_auc = metrics.roc_auc_score(test_y, lr_probs)
# summarize scores
print('No Skill: ROC AUC=%.3f' % (ns_auc))
print('Logistic: ROC AUC=%.3f' % (lr_auc))
# calculate roc curves
ns_fpr, ns_tpr, _ = metrics.roc_curve(test_y, ns_probs)
lr_fpr, lr_tpr, _ = metrics.roc_curve(test_y, lr_probs)
# plot the roc curve for the model
plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')
plt.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')
# axis labels
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
# show the legend
plt.legend()
# show the plot
plt.show()

lr_probs = model.predict_proba(test_x)
# keep probabilities for the positive outcome only
lr_probs = lr_probs[:, 1]
# predict class values
# yhat = model.predict(testX)
lr_precision, lr_recall, _ = metrics.precision_recall_curve(test_y, lr_probs)
lr_auc = metrics.auc(lr_recall, lr_precision)
# summarize scores
print('Logistic: auc=%.3f' % lr_auc)
# plot the precision-recall curves
no_skill = len(test_y[test_y==1]) / len(test_y)
plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')
plt.plot(lr_recall, lr_precision, marker='.', label='Logistic')
# axis labels
plt.xlabel('Recall')
plt.ylabel('Precision')
# show the legend
plt.legend()
# show the plot
plt.show()

"""##6.Please draw the lift curve of models in problem 1: DNN, decision tree and random forest. Please also draw the ideal lift curve and random guess like the above figure."""

def get_cum_gains(df, score, target, title, cm):
    df1 = df[[score,target]].dropna()

    TP = cm[0][0]
    FN = cm[0][1]
    FP = cm[1][0]
    TN = cm[1][1]
    fpr, tpr, thresholds = metrics.roc_curve(df1[target], df1[score])
    # ppr=(tpr*df[target].sum()+fpr*(df[target].count()-df[target].sum()))/df[target].count()
    ppr=(tpr*df[target].sum()+fpr*(df[target].count()-df[target].sum()))
    print(ppr)
    print(tpr)

    ttt = [i*(TP+FN) for i in tpr]
    print(ttt)
    # plt.figure(figsize=(12,4))
    # plt.subplot(1,2,1)
    # metrics.recall_score

    plt.plot(ppr, ttt, label='')
    plt.plot([0,df[target].count()], [0,ttt[-1]])
    plt.plot([0, FN+TN,df[target].count()],[0, ttt[-1],ttt[-1]])
    # plt.plot([0, 1], [0, 1], 'k--')
    # plt.xlim([0.0, 1.0])
    # plt.ylim([0.0, 1.05])
    plt.grid(b=True, which='both', color='0.65',linestyle='-')
    plt.xlabel('%Population')
    plt.ylabel('%Target')
    plt.title(title+'Cumulative Gains Chart')
    plt.legend(loc="lower right")
    # plt.subplot(1,2,2)
    # plt.plot(ppr, tpr/ppr, label='')
    # plt.plot([0, 1], [1, 1], 'k--')
    # plt.grid(b=True, which='both', color='0.65',linestyle='-')
    # plt.xlabel('%Population')
    # plt.ylabel('Lift')
    # plt.title(title+'Lift Curve')

lr_probs = model.predict_proba(test_x)
lr_probs = lr_probs[:, 1]
dict_s = {"score": lr_probs}
cg = pd.DataFrame(dict_s)
test_y_ri = test_y.reset_index()
test_y_ri = test_y_ri['Class']
cg['target'] = test_y_ri
pred_y_test = model.predict(test_x)
confmat = confusion_matrix(test_y, pred_y_test)
get_cum_gains(cg, 'score', 'target', 'title', confmat)