# -*- coding: utf-8 -*-
"""TW_DNN_LSTM .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ocmiJdMvXPKt5QrL9CXLrSy3uzZpH-44
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.utils.data as Data
import torch.optim as optim
import matplotlib.pyplot as plt
import time
from sklearn.metrics import mean_squared_error as mse

def stock_preprocess(stock_pd):

    stock_pd = stock_pd.dropna()
    stock_pd.drop(columns=["Volume"],inplace=True)

    for i in range(len(stock_pd['Date'])):
        temp = stock_pd.loc[i,'Date']
        if '/' in temp:
            # print(covid_pd.loc[i,'Date'])
            
            temp = time.strptime(temp, "%Y/%m/%d")
            nt = time.strftime("%Y-%m-%d", temp)
            # print(nt)
            stock_pd.loc[i,'Date'] = nt

    return stock_pd

df = pd.read_csv('Tw_data.csv')
df = stock_preprocess(df)
df = df[(df['Date'] >= "2020-02-01") & (df['Date'] <= "2020-11-30")]

def normalization(df):
    #df.drop('Date',axis=1,inplace=True)
    col = df.columns.drop('Date')
    for x in col:
        mini = min(df[x])
        maxi = max(df[x])
        if x == 'Close':
            close_max = maxi
            close_min = mini
        df[x] = (df[x] - mini)/(maxi - mini)
    return df, close_max, close_min

def normalization_v2(df):
    #df.drop('Date',axis=1,inplace=True)
    col = df.columns.drop('Date')
    for x in col:
        avg = mean(df[x])
        std = std(df[x])
        if x == 'Close':
            close_avg = maxi
            close_std = mini
        df[x] = (df[x] - avg)/std
    return df, close_avg, close_std

df_norm,  close_max, close_min  = normalization(df.copy())

df_norm

df_norm = df_norm.reset_index(drop=True)

def gen_input_data(data, time_step = 7, future_day = 1):

    data_date = data["Date"]
    # print(data_date)
    data = data.drop(["Date"], axis = 1)
    date_list = []
    train_x, train_y = [], []
    for i in range(data.shape[0]-time_step):
        # print(data_date[i])
        date_list.append(data_date[i+time_step])
        train_x.append(np.array(data.iloc[i:i+time_step]))
        train_y.append(np.array(data.iloc[i+time_step:i+time_step+future_day]["Close"]))
    return np.array(train_x), np.array(train_y), date_list

time_step = 7
train_x, train_y, date_list = gen_input_data(df_norm,time_step)

print(train_x.shape, train_y.shape, len(date_list))

target_date = 0
for idx, d in enumerate(date_list):
    if d > "2020-09-30":
        print(idx, d)
        target_date = idx
        break
print(target_date)

valid_x = train_x[target_date:]
valid_y = train_y[target_date:]
train_x = train_x[:target_date]
train_y = train_y[:target_date]

def shuffle(X,Y):
    np.random.seed(10)
    randomList = np.arange(X.shape[0])
    np.random.shuffle(randomList)
    return X[randomList], Y[randomList]

train_x, train_y = shuffle(train_x, train_y)

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
X_valid = torch.tensor(valid_x,dtype=torch.float32).to(device)
X_train = torch.tensor(train_x,dtype=torch.float32).to(device)
y_valid = torch.tensor(valid_y,dtype=torch.float32).to(device)
y_train = torch.tensor(train_y,dtype=torch.float32).to(device)

class LSTM(nn.Module):
    def __init__(self,input_size, hidden_size, n_layers=1, output_size=1):
        super().__init__()
        self.hidden_size = hidden_size
        self.n_layers = n_layers
        self.lstm0 = nn.LSTM(input_size, hidden_size, n_layers, batch_first=True)  
        self.lstm1 = nn.LSTM(hidden_size, hidden_size, n_layers, batch_first=True)
        self.lstm2 = nn.LSTM(hidden_size, hidden_size, n_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_size).to(device).requires_grad_()
        c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_size).to(device).requires_grad_()
        r_out0, (hidden_state, c) = self.lstm0(x,(h0.detach(),c0.detach()))
        h1 = torch.zeros(self.n_layers, x.size(0), self.hidden_size).to(device).requires_grad_()
        c1 = torch.zeros(self.n_layers, x.size(0), self.hidden_size).to(device).requires_grad_()
        r_out1, (hidden_state1, c1) = self.lstm1(r_out0,(h1.detach(),c1.detach()))
        h2 = torch.zeros(self.n_layers, x.size(0), self.hidden_size).to(device).requires_grad_()
        c2 = torch.zeros(self.n_layers, x.size(0), self.hidden_size).to(device).requires_grad_()
        r_out2, (hidden_state2, c2) = self.lstm2(r_out1,(h2.detach(),c2.detach()))
        outs = []
        for time in range(r_out1.size(1)):
            o = self.fc(r_out2[:, time, :].reshape(-1,self.hidden_size))
            outs.append(o[:].reshape(r_out2.shape[0],-1))
        return torch.stack(outs, dim=1)

input_size = 9
hidden_size = 48
n_layers = 1
output_size = 1

lstm = LSTM(input_size, hidden_size, n_layers, output_size)

epoch = 60
loss_f = nn.MSELoss()
lr = 0.01
optimizer = optim.Adam(lstm.parameters(), lr=lr)
lstm.to(device)
lstm_train_loss, lstm_valid_loss = [], []

for i in range(1,epoch+1):
    print("epoch: {}/{}".format(i,epoch), end = '')
    optimizer.zero_grad()
    lstm_output = lstm(X_train)
    lstm_loss = loss_f(lstm_output[:,time_step-1,0].reshape(-1,1),y_train)
    lstm_train_loss.append(lstm_loss.item())
    print(", train loss: {: 4f}".format(lstm_loss.item()), end='')
    lstm_loss.backward()
    optimizer.step()
    
    with torch.no_grad():
        lstm_output2 = lstm(X_valid)
        lstm_v_loss = loss_f(lstm_output2[:,time_step-1,0].reshape(-1,1), y_valid)
        print(", validation loss: {:4f}.".format(lstm_v_loss.item()))
        lstm_valid_loss.append(lstm_v_loss.item())

predict = lstm_output2[:,time_step-1,0].detach().cpu().numpy().reshape(-1,1)
predict = predict * (close_max - close_min) + close_min
real_index = np.array(df.loc[target_date + time_step:, 'Close']).reshape(-1,1)

mserr = mse(predict, real_index)

plt.plot(predict,color='b',label='predicted')
plt.plot(real_index,color='r',label='real')
plt.legend()
plt.title(f'Stock Index, MSE = {mserr}')
plt.savefig('TW_DNN_LSTM.png')
plt.show()
fig = plt.figure()
plt.plot(lstm_train_loss,color='r',label='train loss')
plt.plot(lstm_valid_loss,color='b',label='validation loss')
plt.legend()
plt.title('Loss')
plt.xlabel('epoch')
plt.savefig('TW_DNN_lstm_loss.png')
plt.show()

torch.save(lstm,'TW_dnn_lstm.pkl')

plt.plot(torch.cat((lstm_output[:,time_step-1,0],lstm_output2[:,time_step-1,0])).detach().cpu().numpy(),color='r',label='predicted price')
plt.plot(torch.cat((y_train,y_valid)).cpu().numpy(),color='b',label='real price')
plt.legend()
plt.title('TW Stock Index')

np.save("DNN", predict)